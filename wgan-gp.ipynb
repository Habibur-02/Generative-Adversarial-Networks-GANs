{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03a1ea45",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-04T18:38:56.840087Z",
     "iopub.status.busy": "2025-09-04T18:38:56.839701Z",
     "iopub.status.idle": "2025-09-04T18:38:58.730528Z",
     "shell.execute_reply": "2025-09-04T18:38:58.729555Z"
    },
    "papermill": {
     "duration": 1.895816,
     "end_time": "2025-09-04T18:38:58.732298",
     "exception": false,
     "start_time": "2025-09-04T18:38:56.836482",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04bce597",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T18:38:58.737772Z",
     "iopub.status.busy": "2025-09-04T18:38:58.737021Z",
     "iopub.status.idle": "2025-09-04T18:44:13.273347Z",
     "shell.execute_reply": "2025-09-04T18:44:13.272501Z"
    },
    "papermill": {
     "duration": 314.540794,
     "end_time": "2025-09-04T18:44:13.275260",
     "exception": false,
     "start_time": "2025-09-04T18:38:58.734466",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:00<00:00, 12.8MB/s]\n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 338kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:00<00:00, 3.17MB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 5.69MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/5] [Batch 0/938] [D loss: 6.6301] [G loss: 0.1289]\n",
      "[Epoch 0/5] [Batch 200/938] [D loss: -5.0097] [G loss: -3.5914]\n",
      "[Epoch 0/5] [Batch 400/938] [D loss: -4.2276] [G loss: -4.3678]\n",
      "[Epoch 0/5] [Batch 600/938] [D loss: -3.8215] [G loss: -3.5722]\n",
      "[Epoch 0/5] [Batch 800/938] [D loss: -3.7748] [G loss: -3.5316]\n",
      "[Epoch 1/5] [Batch 0/938] [D loss: -3.3009] [G loss: -3.5445]\n",
      "[Epoch 1/5] [Batch 200/938] [D loss: -3.4893] [G loss: -2.5006]\n",
      "[Epoch 1/5] [Batch 400/938] [D loss: -3.2513] [G loss: -2.8440]\n",
      "[Epoch 1/5] [Batch 600/938] [D loss: -2.7813] [G loss: -4.0266]\n",
      "[Epoch 1/5] [Batch 800/938] [D loss: -2.8096] [G loss: -4.9633]\n",
      "[Epoch 2/5] [Batch 0/938] [D loss: -2.9883] [G loss: -4.7965]\n",
      "[Epoch 2/5] [Batch 200/938] [D loss: -2.7465] [G loss: -4.8486]\n",
      "[Epoch 2/5] [Batch 400/938] [D loss: -2.7149] [G loss: -4.7887]\n",
      "[Epoch 2/5] [Batch 600/938] [D loss: -3.0011] [G loss: -3.7063]\n",
      "[Epoch 2/5] [Batch 800/938] [D loss: -2.7869] [G loss: -3.7761]\n",
      "[Epoch 3/5] [Batch 0/938] [D loss: -2.8380] [G loss: -3.6683]\n",
      "[Epoch 3/5] [Batch 200/938] [D loss: -3.3342] [G loss: -3.6089]\n",
      "[Epoch 3/5] [Batch 400/938] [D loss: -3.0495] [G loss: -4.1051]\n",
      "[Epoch 3/5] [Batch 600/938] [D loss: -2.8772] [G loss: -3.1442]\n",
      "[Epoch 3/5] [Batch 800/938] [D loss: -2.6848] [G loss: -3.4476]\n",
      "[Epoch 4/5] [Batch 0/938] [D loss: -2.7457] [G loss: -2.8225]\n",
      "[Epoch 4/5] [Batch 200/938] [D loss: -2.3967] [G loss: -2.7001]\n",
      "[Epoch 4/5] [Batch 400/938] [D loss: -3.1281] [G loss: -3.1418]\n",
      "[Epoch 4/5] [Batch 600/938] [D loss: -2.9080] [G loss: -2.4783]\n",
      "[Epoch 4/5] [Batch 800/938] [D loss: -2.3910] [G loss: -3.0734]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "import os\n",
    "\n",
    "# ========== Hyperparameters ==========\n",
    "latent_dim = 100\n",
    "img_size = 28\n",
    "channels = 1\n",
    "batch_size = 64\n",
    "lr = 0.0002\n",
    "epochs = 5\n",
    "lambda_gp = 10     # gradient penalty\n",
    "n_critic = 5       # train critic more\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ========== DataLoader ==========\n",
    "os.makedirs(\"wgan_gp_images\", exist_ok=True)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "dataloader = DataLoader(\n",
    "    datasets.MNIST(\"./data\", train=True, download=True, transform=transform),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# ========== Generator ==========\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, channels * img_size * img_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        return img.view(z.size(0), channels, img_size, img_size)\n",
    "\n",
    "#  Critic \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(channels * img_size * img_size, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img.view(img.size(0), -1))\n",
    "\n",
    "#  Gradient Penalty \n",
    "def gradient_penalty(critic, real_imgs, fake_imgs):\n",
    "    batch_size = real_imgs.size(0)\n",
    "    epsilon = torch.rand(batch_size, 1, 1, 1, device=device, requires_grad=True)\n",
    "    interpolates = (epsilon * real_imgs + (1 - epsilon) * fake_imgs).requires_grad_(True)\n",
    "\n",
    "    d_interpolates = critic(interpolates)\n",
    "    fake = torch.ones(batch_size, 1, device=device, requires_grad=False)\n",
    "\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "\n",
    "    gradients = gradients.view(batch_size, -1)\n",
    "    gp = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gp\n",
    "\n",
    "#  Initialize \n",
    "generator = Generator().to(device)\n",
    "critic = Critic().to(device)\n",
    "\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.9))\n",
    "optimizer_C = optim.Adam(critic.parameters(), lr=lr, betas=(0.5, 0.9))\n",
    "\n",
    "#  Training \n",
    "for epoch in range(epochs):\n",
    "    for i, (imgs, _) in enumerate(dataloader):\n",
    "\n",
    "        real_imgs = imgs.to(device)\n",
    "\n",
    "        #  Train Critic \n",
    "        for _ in range(n_critic):\n",
    "            z = torch.randn(imgs.size(0), latent_dim).to(device)\n",
    "            fake_imgs = generator(z).detach()\n",
    "\n",
    "            real_validity = critic(real_imgs)\n",
    "            fake_validity = critic(fake_imgs)\n",
    "\n",
    "            gp = gradient_penalty(critic, real_imgs, fake_imgs)\n",
    "\n",
    "            loss_C = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gp\n",
    "\n",
    "            optimizer_C.zero_grad()\n",
    "            loss_C.backward()\n",
    "            optimizer_C.step()\n",
    "\n",
    "        # ================== Train Generator \n",
    "        z = torch.randn(imgs.size(0), latent_dim).to(device)\n",
    "        gen_imgs = generator(z)\n",
    "        loss_G = -torch.mean(critic(gen_imgs))\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # Print log\n",
    "        if i % 200 == 0:\n",
    "            print(f\"[Epoch {epoch}/{epochs}] [Batch {i}/{len(dataloader)}] \"\n",
    "                  f\"[D loss: {loss_C.item():.4f}] [G loss: {loss_G.item():.4f}]\")\n",
    "\n",
    "    save_image(gen_imgs[:25], f\"wgan_gp_images/{epoch}.png\", nrow=5, normalize=True)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 324.918292,
   "end_time": "2025-09-04T18:44:16.080504",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-04T18:38:51.162212",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
