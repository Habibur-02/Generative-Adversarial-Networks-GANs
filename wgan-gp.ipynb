{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98341ea6",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-22T18:53:03.094586Z",
     "iopub.status.busy": "2025-08-22T18:53:03.094221Z",
     "iopub.status.idle": "2025-08-22T18:53:05.339782Z",
     "shell.execute_reply": "2025-08-22T18:53:05.338570Z"
    },
    "papermill": {
     "duration": 2.251186,
     "end_time": "2025-08-22T18:53:05.341827",
     "exception": false,
     "start_time": "2025-08-22T18:53:03.090641",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66feb511",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T18:53:05.347601Z",
     "iopub.status.busy": "2025-08-22T18:53:05.347123Z",
     "iopub.status.idle": "2025-08-22T18:58:43.009643Z",
     "shell.execute_reply": "2025-08-22T18:58:43.008718Z"
    },
    "papermill": {
     "duration": 337.667434,
     "end_time": "2025-08-22T18:58:43.011597",
     "exception": false,
     "start_time": "2025-08-22T18:53:05.344163",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:00<00:00, 17.7MB/s]\n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 483kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.45MB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 3.21MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/5] [Batch 0/938] [D loss: 6.4260] [G loss: 0.1446]\n",
      "[Epoch 0/5] [Batch 200/938] [D loss: -4.8134] [G loss: -1.4233]\n",
      "[Epoch 0/5] [Batch 400/938] [D loss: -4.0210] [G loss: -5.5945]\n",
      "[Epoch 0/5] [Batch 600/938] [D loss: -3.9724] [G loss: -2.2171]\n",
      "[Epoch 0/5] [Batch 800/938] [D loss: -3.3473] [G loss: -2.7137]\n",
      "[Epoch 1/5] [Batch 0/938] [D loss: -3.0975] [G loss: -3.1838]\n",
      "[Epoch 1/5] [Batch 200/938] [D loss: -2.9269] [G loss: -4.1302]\n",
      "[Epoch 1/5] [Batch 400/938] [D loss: -2.8355] [G loss: -4.4728]\n",
      "[Epoch 1/5] [Batch 600/938] [D loss: -2.6389] [G loss: -5.8737]\n",
      "[Epoch 1/5] [Batch 800/938] [D loss: -2.5691] [G loss: -4.8553]\n",
      "[Epoch 2/5] [Batch 0/938] [D loss: -2.4456] [G loss: -4.8023]\n",
      "[Epoch 2/5] [Batch 200/938] [D loss: -2.5475] [G loss: -5.5885]\n",
      "[Epoch 2/5] [Batch 400/938] [D loss: -2.6041] [G loss: -5.7261]\n",
      "[Epoch 2/5] [Batch 600/938] [D loss: -2.5054] [G loss: -5.4324]\n",
      "[Epoch 2/5] [Batch 800/938] [D loss: -2.3573] [G loss: -4.8571]\n",
      "[Epoch 3/5] [Batch 0/938] [D loss: -2.2095] [G loss: -3.4742]\n",
      "[Epoch 3/5] [Batch 200/938] [D loss: -2.8299] [G loss: -3.2876]\n",
      "[Epoch 3/5] [Batch 400/938] [D loss: -2.9685] [G loss: -1.3647]\n",
      "[Epoch 3/5] [Batch 600/938] [D loss: -2.4428] [G loss: -2.9287]\n",
      "[Epoch 3/5] [Batch 800/938] [D loss: -2.8232] [G loss: -3.6233]\n",
      "[Epoch 4/5] [Batch 0/938] [D loss: -2.6724] [G loss: -3.5499]\n",
      "[Epoch 4/5] [Batch 200/938] [D loss: -2.7054] [G loss: -4.3203]\n",
      "[Epoch 4/5] [Batch 400/938] [D loss: -2.5322] [G loss: -2.4070]\n",
      "[Epoch 4/5] [Batch 600/938] [D loss: -2.8704] [G loss: -1.1825]\n",
      "[Epoch 4/5] [Batch 800/938] [D loss: -2.6386] [G loss: -3.7704]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "import os\n",
    "\n",
    "# ========== Hyperparameters ==========\n",
    "latent_dim = 100\n",
    "img_size = 28\n",
    "channels = 1\n",
    "batch_size = 64\n",
    "lr = 0.0002\n",
    "epochs = 5\n",
    "lambda_gp = 10     # gradient penalty\n",
    "n_critic = 5       # train critic more\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ========== DataLoader ==========\n",
    "os.makedirs(\"wgan_gp_images\", exist_ok=True)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "dataloader = DataLoader(\n",
    "    datasets.MNIST(\"./data\", train=True, download=True, transform=transform),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# ========== Generator ==========\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, channels * img_size * img_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        return img.view(z.size(0), channels, img_size, img_size)\n",
    "\n",
    "#  Critic \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(channels * img_size * img_size, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img.view(img.size(0), -1))\n",
    "\n",
    "#  Gradient Penalty \n",
    "def gradient_penalty(critic, real_imgs, fake_imgs):\n",
    "    batch_size = real_imgs.size(0)\n",
    "    epsilon = torch.rand(batch_size, 1, 1, 1, device=device, requires_grad=True)\n",
    "    interpolates = (epsilon * real_imgs + (1 - epsilon) * fake_imgs).requires_grad_(True)\n",
    "\n",
    "    d_interpolates = critic(interpolates)\n",
    "    fake = torch.ones(batch_size, 1, device=device, requires_grad=False)\n",
    "\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "\n",
    "    gradients = gradients.view(batch_size, -1)\n",
    "    gp = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gp\n",
    "\n",
    "#  Initialize \n",
    "generator = Generator().to(device)\n",
    "critic = Critic().to(device)\n",
    "\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.9))\n",
    "optimizer_C = optim.Adam(critic.parameters(), lr=lr, betas=(0.5, 0.9))\n",
    "\n",
    "#  Training \n",
    "for epoch in range(epochs):\n",
    "    for i, (imgs, _) in enumerate(dataloader):\n",
    "\n",
    "        real_imgs = imgs.to(device)\n",
    "\n",
    "        #  Train Critic \n",
    "        for _ in range(n_critic):\n",
    "            z = torch.randn(imgs.size(0), latent_dim).to(device)\n",
    "            fake_imgs = generator(z).detach()\n",
    "\n",
    "            real_validity = critic(real_imgs)\n",
    "            fake_validity = critic(fake_imgs)\n",
    "\n",
    "            gp = gradient_penalty(critic, real_imgs, fake_imgs)\n",
    "\n",
    "            loss_C = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gp\n",
    "\n",
    "            optimizer_C.zero_grad()\n",
    "            loss_C.backward()\n",
    "            optimizer_C.step()\n",
    "\n",
    "        # ================== Train Generator \n",
    "        z = torch.randn(imgs.size(0), latent_dim).to(device)\n",
    "        gen_imgs = generator(z)\n",
    "        loss_G = -torch.mean(critic(gen_imgs))\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # Print log\n",
    "        if i % 200 == 0:\n",
    "            print(f\"[Epoch {epoch}/{epochs}] [Batch {i}/{len(dataloader)}] \"\n",
    "                  f\"[D loss: {loss_C.item():.4f}] [G loss: {loss_G.item():.4f}]\")\n",
    "\n",
    "    save_image(gen_imgs[:25], f\"wgan_gp_images/{epoch}.png\", nrow=5, normalize=True)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 349.212886,
   "end_time": "2025-08-22T18:58:45.899708",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-22T18:52:56.686822",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
