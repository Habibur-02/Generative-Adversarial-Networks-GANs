{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c8a99ee",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-08T15:56:56.443379Z",
     "iopub.status.busy": "2025-08-08T15:56:56.442684Z",
     "iopub.status.idle": "2025-08-08T15:56:57.928716Z",
     "shell.execute_reply": "2025-08-08T15:56:57.927846Z"
    },
    "papermill": {
     "duration": 1.491075,
     "end_time": "2025-08-08T15:56:57.930188",
     "exception": false,
     "start_time": "2025-08-08T15:56:56.439113",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/dataset/20240427_161502.jpg\n",
      "/kaggle/input/dataset/IMG_5699.jpeg\n",
      "/kaggle/input/dataset/20240427_161434.jpg\n",
      "/kaggle/input/dataset/20240427_161527.jpg\n",
      "/kaggle/input/dataset/IMG-3736.jpg\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "827c3a56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T15:56:57.937843Z",
     "iopub.status.busy": "2025-08-08T15:56:57.937517Z",
     "iopub.status.idle": "2025-08-08T15:56:58.425070Z",
     "shell.execute_reply": "2025-08-08T15:56:58.424263Z"
    },
    "papermill": {
     "duration": 0.491858,
     "end_time": "2025-08-08T15:56:58.426532",
     "exception": false,
     "start_time": "2025-08-08T15:56:57.934674",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Internal folder created with images.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Make internal folder\n",
    "os.makedirs(\"/kaggle/working/dataset/all_images\", exist_ok=True)\n",
    "\n",
    "# Copy all images from your input dataset to the new internal folder\n",
    "for file in os.listdir(\"/kaggle/input/dataset\"):\n",
    "    src = os.path.join(\"/kaggle/input/dataset\", file)\n",
    "    dst = os.path.join(\"/kaggle/working/dataset/all_images\", file)\n",
    "    if os.path.isfile(src):  # only copy files\n",
    "        shutil.copy(src, dst)\n",
    "\n",
    "print(\"Internal folder created with images.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f2c10f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T15:56:58.434038Z",
     "iopub.status.busy": "2025-08-08T15:56:58.433784Z",
     "iopub.status.idle": "2025-08-08T15:57:07.635168Z",
     "shell.execute_reply": "2025-08-08T15:57:07.634283Z"
    },
    "papermill": {
     "duration": 9.206707,
     "end_time": "2025-08-08T15:57:07.636739",
     "exception": false,
     "start_time": "2025-08-08T15:56:58.430032",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms \n",
    "from torchvision.utils import save_image\n",
    "import os\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "\n",
    "latent_dim = 100\n",
    "img_channels = 3\n",
    "img_size = 32\n",
    "batch_size = 1\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "epochs = 100\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "os.makedirs(\"dcgan_outputs\", exist_ok=True)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size,img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "dataset = ImageFolder(root='/kaggle/working/dataset', transform=transform)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b964b8b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T15:57:07.641961Z",
     "iopub.status.busy": "2025-08-08T15:57:07.641636Z",
     "iopub.status.idle": "2025-08-08T15:57:07.883812Z",
     "shell.execute_reply": "2025-08-08T15:57:07.882994Z"
    },
    "papermill": {
     "duration": 0.246412,
     "end_time": "2025-08-08T15:57:07.885351",
     "exception": false,
     "start_time": "2025-08-08T15:57:07.638939",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (5): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (8): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1))\n",
       "    (9): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model=nn.Sequential(\n",
    "            nn.ConvTranspose2d(latent_dim, 512, 4, 1, 0), #in_chan,out_chan, kernel, stride, padd\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2,1), #in_chan,out_chan, kernel, stride, padd\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1),       # (B,128,16,16)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(128, img_channels, 4, 2, 1),# (B,3,32,32)\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self,z):\n",
    "        return self.model(z)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model= nn.Sequential(\n",
    "            nn.Conv2d(img_channels, 128, 4,2,1), #[B,128, 16, 16]\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(128,256,4,2,1),   #8*8\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 512, 4, 2, 1),  #4*4\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(512,1,4,1,0),\n",
    "            nn.Sigmoid()\n",
    "\n",
    "\n",
    "        )\n",
    "    def forward(self, img):\n",
    "        out=self.model(img)\n",
    "        return out.view(-1,1)\n",
    "\n",
    "\n",
    "model_Gen= Generator().to(device)\n",
    "model_Gen\n",
    "\n",
    "model_Dis=Discriminator().to(device)\n",
    "\n",
    "model_Dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef8f8bfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T15:57:07.891515Z",
     "iopub.status.busy": "2025-08-08T15:57:07.891299Z",
     "iopub.status.idle": "2025-08-08T15:57:07.906517Z",
     "shell.execute_reply": "2025-08-08T15:57:07.905954Z"
    },
    "papermill": {
     "duration": 0.018949,
     "end_time": "2025-08-08T15:57:07.907536",
     "exception": false,
     "start_time": "2025-08-08T15:57:07.888587",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_fn=nn.BCELoss()\n",
    "\n",
    "optimizer_G=torch.optim.Adam(params=model_Gen.parameters(),lr=lr, betas=(beta1, 0.999))\n",
    "optimizer_D=torch.optim.Adam(params=model_Dis.parameters(),lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "\n",
    "fixed_noise=torch.randn(64, latent_dim, 1, 1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22ec2bce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T15:57:07.912236Z",
     "iopub.status.busy": "2025-08-08T15:57:07.911987Z",
     "iopub.status.idle": "2025-08-08T15:57:07.915622Z",
     "shell.execute_reply": "2025-08-08T15:57:07.914977Z"
    },
    "papermill": {
     "duration": 0.00719,
     "end_time": "2025-08-08T15:57:07.916677",
     "exception": false,
     "start_time": "2025-08-08T15:57:07.909487",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# epochs=1\n",
    "# k=0\n",
    "# for i, (imgs, _) in enumerate(train_loader):\n",
    "#     if i>1:\n",
    "#         break\n",
    "#     imgs=imgs.to(device)\n",
    "#     batch=imgs.size(0)\n",
    "#     real_label=torch.full((batch, 1), 0.9, device=device)\n",
    "#     fake_label=torch.zeros(batch, 1).to(device)\n",
    "#     # real_out=model_Dis()\n",
    "#     print(imgs.shape)\n",
    "#     #trainging discriminator\n",
    "#     optimizer_D.zero_grad()\n",
    "#     z=torch.randn(batch, latent_dim, 1, 1, device=device)\n",
    "#     real_out=model_Dis(imgs)\n",
    "#     real_loss=loss_fn(real_out, real_label)\n",
    "\n",
    "#     fake_imgs=model_Gen(z)\n",
    "#     fake_out=model_Dis(fake_imgs.detach())\n",
    "#     fake_loss=loss_fn(fake_out, fake_label)\n",
    "\n",
    "#     total_loss=real_loss+fake_loss\n",
    "#     print(total_loss.item())\n",
    "#     total_loss.backward()\n",
    "#     optimizer_D.step()\n",
    "#     #Generator training\n",
    "#     optimizer_G.zero_grad()\n",
    "\n",
    "\n",
    "#     gen_img=model_Gen(z)\n",
    "#     dis_out_val=model_Dis(gen_img)\n",
    "\n",
    "#     gen_loss=loss_fn(dis_out_val,real_label)\n",
    "#     print(gen_loss.item())\n",
    "#     gen_loss.backward()\n",
    "#     optimizer_G.step()\n",
    "\n",
    "\n",
    "#     if i%1==0:\n",
    "#         print(f\"i->{i} D[{total_loss.item()}] G[{gen_loss.item()}]\")\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         fake_sample=model_Gen(fixed_noise).detach().cpu()\n",
    "#         save_image(fake_sample, f\"dcgan_outputs/fake_{epochs:03d}_{k:04d}.png\", normalize=True)\n",
    "\n",
    "\n",
    "\n",
    "# torch.save(model_Gen.state_dict(),\"dcgan_generate.pth\")\n",
    "# torch.save(model_Dis.state_dict(),\"dcgan_discriminator.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d21452e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T15:57:07.921401Z",
     "iopub.status.busy": "2025-08-08T15:57:07.921192Z",
     "iopub.status.idle": "2025-08-08T15:58:55.213303Z",
     "shell.execute_reply": "2025-08-08T15:58:55.212335Z"
    },
    "papermill": {
     "duration": 107.295943,
     "end_time": "2025-08-08T15:58:55.214534",
     "exception": false,
     "start_time": "2025-08-08T15:57:07.918591",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/100] [Batch 0/5] [D loss: 1.2498] [G loss: 7.6156]\n",
      "[Epoch 1/100] [Batch 1/5] [D loss: 0.8008] [G loss: 8.2821]\n",
      "[Epoch 1/100] [Batch 2/5] [D loss: 1.4050] [G loss: 5.7704]\n",
      "[Epoch 1/100] [Batch 3/5] [D loss: 2.1668] [G loss: 3.7015]\n",
      "[Epoch 1/100] [Batch 4/5] [D loss: 1.0039] [G loss: 6.6485]\n",
      "[Epoch 2/100] [Batch 0/5] [D loss: 0.5430] [G loss: 5.3008]\n",
      "[Epoch 2/100] [Batch 1/5] [D loss: 1.1481] [G loss: 4.9499]\n",
      "[Epoch 2/100] [Batch 2/5] [D loss: 0.7286] [G loss: 6.6641]\n",
      "[Epoch 2/100] [Batch 3/5] [D loss: 1.0079] [G loss: 5.0679]\n",
      "[Epoch 2/100] [Batch 4/5] [D loss: 0.9108] [G loss: 5.0171]\n",
      "[Epoch 3/100] [Batch 0/5] [D loss: 0.9041] [G loss: 6.9457]\n",
      "[Epoch 3/100] [Batch 1/5] [D loss: 0.5642] [G loss: 5.2304]\n",
      "[Epoch 3/100] [Batch 2/5] [D loss: 0.3749] [G loss: 4.7262]\n",
      "[Epoch 3/100] [Batch 3/5] [D loss: 1.9815] [G loss: 4.3939]\n",
      "[Epoch 3/100] [Batch 4/5] [D loss: 0.9562] [G loss: 6.1994]\n",
      "[Epoch 4/100] [Batch 0/5] [D loss: 0.4996] [G loss: 5.4762]\n",
      "[Epoch 4/100] [Batch 1/5] [D loss: 0.6003] [G loss: 5.1841]\n",
      "[Epoch 4/100] [Batch 2/5] [D loss: 0.5604] [G loss: 4.4222]\n",
      "[Epoch 4/100] [Batch 3/5] [D loss: 1.0007] [G loss: 5.6375]\n",
      "[Epoch 4/100] [Batch 4/5] [D loss: 0.3841] [G loss: 5.1411]\n",
      "[Epoch 5/100] [Batch 0/5] [D loss: 0.6293] [G loss: 4.9336]\n",
      "[Epoch 5/100] [Batch 1/5] [D loss: 0.8438] [G loss: 4.8719]\n",
      "[Epoch 5/100] [Batch 2/5] [D loss: 1.1419] [G loss: 4.5232]\n",
      "[Epoch 5/100] [Batch 3/5] [D loss: 0.5422] [G loss: 6.4404]\n",
      "[Epoch 5/100] [Batch 4/5] [D loss: 0.4792] [G loss: 6.4245]\n",
      "[Epoch 6/100] [Batch 0/5] [D loss: 0.3912] [G loss: 4.9532]\n",
      "[Epoch 6/100] [Batch 1/5] [D loss: 0.8394] [G loss: 4.5391]\n",
      "[Epoch 6/100] [Batch 2/5] [D loss: 0.5277] [G loss: 4.7757]\n",
      "[Epoch 6/100] [Batch 3/5] [D loss: 0.6174] [G loss: 4.8290]\n",
      "[Epoch 6/100] [Batch 4/5] [D loss: 0.4843] [G loss: 4.9601]\n",
      "[Epoch 7/100] [Batch 0/5] [D loss: 0.6180] [G loss: 4.5482]\n",
      "[Epoch 7/100] [Batch 1/5] [D loss: 0.6616] [G loss: 4.4682]\n",
      "[Epoch 7/100] [Batch 2/5] [D loss: 0.4485] [G loss: 5.5012]\n",
      "[Epoch 7/100] [Batch 3/5] [D loss: 0.3704] [G loss: 5.8684]\n",
      "[Epoch 7/100] [Batch 4/5] [D loss: 0.4211] [G loss: 5.5441]\n",
      "[Epoch 8/100] [Batch 0/5] [D loss: 0.3760] [G loss: 4.7318]\n",
      "[Epoch 8/100] [Batch 1/5] [D loss: 0.4046] [G loss: 4.4868]\n",
      "[Epoch 8/100] [Batch 2/5] [D loss: 0.3921] [G loss: 4.4974]\n",
      "[Epoch 8/100] [Batch 3/5] [D loss: 0.5669] [G loss: 5.4491]\n",
      "[Epoch 8/100] [Batch 4/5] [D loss: 0.4012] [G loss: 5.2486]\n",
      "[Epoch 9/100] [Batch 0/5] [D loss: 0.3384] [G loss: 5.7992]\n",
      "[Epoch 9/100] [Batch 1/5] [D loss: 0.3969] [G loss: 4.7022]\n",
      "[Epoch 9/100] [Batch 2/5] [D loss: 0.3819] [G loss: 5.5763]\n",
      "[Epoch 9/100] [Batch 3/5] [D loss: 0.6988] [G loss: 3.4449]\n",
      "[Epoch 9/100] [Batch 4/5] [D loss: 0.3911] [G loss: 4.4025]\n",
      "[Epoch 10/100] [Batch 0/5] [D loss: 0.6107] [G loss: 6.0782]\n",
      "[Epoch 10/100] [Batch 1/5] [D loss: 0.3607] [G loss: 5.3086]\n",
      "[Epoch 10/100] [Batch 2/5] [D loss: 0.3406] [G loss: 6.2682]\n",
      "[Epoch 10/100] [Batch 3/5] [D loss: 0.3275] [G loss: 6.4558]\n",
      "[Epoch 10/100] [Batch 4/5] [D loss: 0.4825] [G loss: 4.6824]\n",
      "[Epoch 11/100] [Batch 0/5] [D loss: 0.3989] [G loss: 4.8159]\n",
      "[Epoch 11/100] [Batch 1/5] [D loss: 0.3443] [G loss: 5.2575]\n",
      "[Epoch 11/100] [Batch 2/5] [D loss: 0.3919] [G loss: 4.5331]\n",
      "[Epoch 11/100] [Batch 3/5] [D loss: 0.3564] [G loss: 5.3620]\n",
      "[Epoch 11/100] [Batch 4/5] [D loss: 0.3270] [G loss: 5.9188]\n",
      "[Epoch 12/100] [Batch 0/5] [D loss: 0.3310] [G loss: 4.9398]\n",
      "[Epoch 12/100] [Batch 1/5] [D loss: 0.3759] [G loss: 4.5366]\n",
      "[Epoch 12/100] [Batch 2/5] [D loss: 0.5831] [G loss: 7.2470]\n",
      "[Epoch 12/100] [Batch 3/5] [D loss: 0.3476] [G loss: 7.8503]\n",
      "[Epoch 12/100] [Batch 4/5] [D loss: 0.5551] [G loss: 6.6356]\n",
      "[Epoch 13/100] [Batch 0/5] [D loss: 0.4136] [G loss: 7.3871]\n",
      "[Epoch 13/100] [Batch 1/5] [D loss: 0.3355] [G loss: 7.6613]\n",
      "[Epoch 13/100] [Batch 2/5] [D loss: 0.4255] [G loss: 3.8729]\n",
      "[Epoch 13/100] [Batch 3/5] [D loss: 0.3623] [G loss: 4.5034]\n",
      "[Epoch 13/100] [Batch 4/5] [D loss: 0.9746] [G loss: 4.7130]\n",
      "[Epoch 14/100] [Batch 0/5] [D loss: 0.3639] [G loss: 4.9261]\n",
      "[Epoch 14/100] [Batch 1/5] [D loss: 0.7209] [G loss: 7.2542]\n",
      "[Epoch 14/100] [Batch 2/5] [D loss: 0.3938] [G loss: 5.1188]\n",
      "[Epoch 14/100] [Batch 3/5] [D loss: 0.4398] [G loss: 6.1289]\n",
      "[Epoch 14/100] [Batch 4/5] [D loss: 0.3750] [G loss: 10.6053]\n",
      "[Epoch 15/100] [Batch 0/5] [D loss: 0.4462] [G loss: 7.6496]\n",
      "[Epoch 15/100] [Batch 1/5] [D loss: 0.3373] [G loss: 4.5836]\n",
      "[Epoch 15/100] [Batch 2/5] [D loss: 0.3472] [G loss: 6.6846]\n",
      "[Epoch 15/100] [Batch 3/5] [D loss: 0.3742] [G loss: 5.2873]\n",
      "[Epoch 15/100] [Batch 4/5] [D loss: 0.3733] [G loss: 7.0276]\n",
      "[Epoch 16/100] [Batch 0/5] [D loss: 0.4776] [G loss: 4.4393]\n",
      "[Epoch 16/100] [Batch 1/5] [D loss: 0.6740] [G loss: 8.8698]\n",
      "[Epoch 16/100] [Batch 2/5] [D loss: 0.4870] [G loss: 9.3522]\n",
      "[Epoch 16/100] [Batch 3/5] [D loss: 0.6114] [G loss: 8.5883]\n",
      "[Epoch 16/100] [Batch 4/5] [D loss: 0.3585] [G loss: 8.7245]\n",
      "[Epoch 17/100] [Batch 0/5] [D loss: 0.7269] [G loss: 8.7573]\n",
      "[Epoch 17/100] [Batch 1/5] [D loss: 0.3521] [G loss: 9.6664]\n",
      "[Epoch 17/100] [Batch 2/5] [D loss: 0.4526] [G loss: 5.4693]\n",
      "[Epoch 17/100] [Batch 3/5] [D loss: 0.5326] [G loss: 7.5664]\n",
      "[Epoch 17/100] [Batch 4/5] [D loss: 0.3498] [G loss: 4.4568]\n",
      "[Epoch 18/100] [Batch 0/5] [D loss: 0.6721] [G loss: 4.8050]\n",
      "[Epoch 18/100] [Batch 1/5] [D loss: 0.3731] [G loss: 4.2009]\n",
      "[Epoch 18/100] [Batch 2/5] [D loss: 0.3642] [G loss: 4.9065]\n",
      "[Epoch 18/100] [Batch 3/5] [D loss: 0.8735] [G loss: 5.0269]\n",
      "[Epoch 18/100] [Batch 4/5] [D loss: 0.3860] [G loss: 4.9537]\n",
      "[Epoch 19/100] [Batch 0/5] [D loss: 0.3495] [G loss: 4.9640]\n",
      "[Epoch 19/100] [Batch 1/5] [D loss: 0.3393] [G loss: 4.7434]\n",
      "[Epoch 19/100] [Batch 2/5] [D loss: 0.7476] [G loss: 6.9338]\n",
      "[Epoch 19/100] [Batch 3/5] [D loss: 0.3664] [G loss: 6.2026]\n",
      "[Epoch 19/100] [Batch 4/5] [D loss: 0.3656] [G loss: 8.9853]\n",
      "[Epoch 20/100] [Batch 0/5] [D loss: 0.3638] [G loss: 7.6215]\n",
      "[Epoch 20/100] [Batch 1/5] [D loss: 0.3378] [G loss: 7.8464]\n",
      "[Epoch 20/100] [Batch 2/5] [D loss: 0.3434] [G loss: 6.8107]\n",
      "[Epoch 20/100] [Batch 3/5] [D loss: 0.3587] [G loss: 6.2275]\n",
      "[Epoch 20/100] [Batch 4/5] [D loss: 0.3498] [G loss: 4.3567]\n",
      "[Epoch 21/100] [Batch 0/5] [D loss: 0.3588] [G loss: 6.8716]\n",
      "[Epoch 21/100] [Batch 1/5] [D loss: 0.3723] [G loss: 4.2769]\n",
      "[Epoch 21/100] [Batch 2/5] [D loss: 0.3796] [G loss: 7.7293]\n",
      "[Epoch 21/100] [Batch 3/5] [D loss: 0.3698] [G loss: 7.6130]\n",
      "[Epoch 21/100] [Batch 4/5] [D loss: 0.3457] [G loss: 5.0015]\n",
      "[Epoch 22/100] [Batch 0/5] [D loss: 0.3442] [G loss: 5.7583]\n",
      "[Epoch 22/100] [Batch 1/5] [D loss: 0.3527] [G loss: 4.4514]\n",
      "[Epoch 22/100] [Batch 2/5] [D loss: 0.3931] [G loss: 4.8725]\n",
      "[Epoch 22/100] [Batch 3/5] [D loss: 0.3391] [G loss: 6.1686]\n",
      "[Epoch 22/100] [Batch 4/5] [D loss: 0.3285] [G loss: 5.6788]\n",
      "[Epoch 23/100] [Batch 0/5] [D loss: 0.3560] [G loss: 5.2010]\n",
      "[Epoch 23/100] [Batch 1/5] [D loss: 0.3529] [G loss: 4.4453]\n",
      "[Epoch 23/100] [Batch 2/5] [D loss: 0.3753] [G loss: 4.8821]\n",
      "[Epoch 23/100] [Batch 3/5] [D loss: 0.3282] [G loss: 6.3126]\n",
      "[Epoch 23/100] [Batch 4/5] [D loss: 0.3274] [G loss: 6.2099]\n",
      "[Epoch 24/100] [Batch 0/5] [D loss: 0.3296] [G loss: 6.3188]\n",
      "[Epoch 24/100] [Batch 1/5] [D loss: 0.3302] [G loss: 5.5142]\n",
      "[Epoch 24/100] [Batch 2/5] [D loss: 0.3620] [G loss: 5.9492]\n",
      "[Epoch 24/100] [Batch 3/5] [D loss: 0.3474] [G loss: 4.5073]\n",
      "[Epoch 24/100] [Batch 4/5] [D loss: 0.3305] [G loss: 5.1381]\n",
      "[Epoch 25/100] [Batch 0/5] [D loss: 0.3469] [G loss: 4.5910]\n",
      "[Epoch 25/100] [Batch 1/5] [D loss: 0.4610] [G loss: 5.2783]\n",
      "[Epoch 25/100] [Batch 2/5] [D loss: 0.3596] [G loss: 5.4515]\n",
      "[Epoch 25/100] [Batch 3/5] [D loss: 0.3308] [G loss: 9.5310]\n",
      "[Epoch 25/100] [Batch 4/5] [D loss: 0.3266] [G loss: 7.0686]\n",
      "[Epoch 26/100] [Batch 0/5] [D loss: 0.3328] [G loss: 6.7564]\n",
      "[Epoch 26/100] [Batch 1/5] [D loss: 0.3325] [G loss: 6.9944]\n",
      "[Epoch 26/100] [Batch 2/5] [D loss: 0.3285] [G loss: 5.2647]\n",
      "[Epoch 26/100] [Batch 3/5] [D loss: 0.3728] [G loss: 4.3617]\n",
      "[Epoch 26/100] [Batch 4/5] [D loss: 0.4126] [G loss: 5.2634]\n",
      "[Epoch 27/100] [Batch 0/5] [D loss: 0.3458] [G loss: 7.6125]\n",
      "[Epoch 27/100] [Batch 1/5] [D loss: 0.3252] [G loss: 9.1795]\n",
      "[Epoch 27/100] [Batch 2/5] [D loss: 0.3309] [G loss: 6.7859]\n",
      "[Epoch 27/100] [Batch 3/5] [D loss: 0.4307] [G loss: 7.4979]\n",
      "[Epoch 27/100] [Batch 4/5] [D loss: 0.3273] [G loss: 5.2440]\n",
      "[Epoch 28/100] [Batch 0/5] [D loss: 0.5315] [G loss: 4.8801]\n",
      "[Epoch 28/100] [Batch 1/5] [D loss: 0.3279] [G loss: 6.2452]\n",
      "[Epoch 28/100] [Batch 2/5] [D loss: 0.3625] [G loss: 4.7235]\n",
      "[Epoch 28/100] [Batch 3/5] [D loss: 0.3368] [G loss: 5.0816]\n",
      "[Epoch 28/100] [Batch 4/5] [D loss: 0.3257] [G loss: 7.7366]\n",
      "[Epoch 29/100] [Batch 0/5] [D loss: 0.3281] [G loss: 5.8030]\n",
      "[Epoch 29/100] [Batch 1/5] [D loss: 0.3254] [G loss: 8.2352]\n",
      "[Epoch 29/100] [Batch 2/5] [D loss: 0.3265] [G loss: 7.6758]\n",
      "[Epoch 29/100] [Batch 3/5] [D loss: 0.3290] [G loss: 5.4193]\n",
      "[Epoch 29/100] [Batch 4/5] [D loss: 0.3598] [G loss: 6.8123]\n",
      "[Epoch 30/100] [Batch 0/5] [D loss: 0.3446] [G loss: 4.6100]\n",
      "[Epoch 30/100] [Batch 1/5] [D loss: 0.3389] [G loss: 5.2652]\n",
      "[Epoch 30/100] [Batch 2/5] [D loss: 0.3272] [G loss: 5.8315]\n",
      "[Epoch 30/100] [Batch 3/5] [D loss: 0.3307] [G loss: 5.0739]\n",
      "[Epoch 30/100] [Batch 4/5] [D loss: 0.3293] [G loss: 5.2853]\n",
      "[Epoch 31/100] [Batch 0/5] [D loss: 0.3316] [G loss: 5.7865]\n",
      "[Epoch 31/100] [Batch 1/5] [D loss: 0.3311] [G loss: 6.5555]\n",
      "[Epoch 31/100] [Batch 2/5] [D loss: 0.3837] [G loss: 5.4489]\n",
      "[Epoch 31/100] [Batch 3/5] [D loss: 0.3267] [G loss: 8.3422]\n",
      "[Epoch 31/100] [Batch 4/5] [D loss: 0.3254] [G loss: 9.3480]\n",
      "[Epoch 32/100] [Batch 0/5] [D loss: 0.3257] [G loss: 7.0854]\n",
      "[Epoch 32/100] [Batch 1/5] [D loss: 0.4140] [G loss: 6.2580]\n",
      "[Epoch 32/100] [Batch 2/5] [D loss: 0.3312] [G loss: 5.1534]\n",
      "[Epoch 32/100] [Batch 3/5] [D loss: 0.3372] [G loss: 5.0636]\n",
      "[Epoch 32/100] [Batch 4/5] [D loss: 0.3362] [G loss: 6.4637]\n",
      "[Epoch 33/100] [Batch 0/5] [D loss: 0.5784] [G loss: 6.1693]\n",
      "[Epoch 33/100] [Batch 1/5] [D loss: 0.3282] [G loss: 5.6070]\n",
      "[Epoch 33/100] [Batch 2/5] [D loss: 0.3516] [G loss: 5.4456]\n",
      "[Epoch 33/100] [Batch 3/5] [D loss: 0.3523] [G loss: 6.4370]\n",
      "[Epoch 33/100] [Batch 4/5] [D loss: 0.3435] [G loss: 4.5789]\n",
      "[Epoch 34/100] [Batch 0/5] [D loss: 0.4033] [G loss: 4.7883]\n",
      "[Epoch 34/100] [Batch 1/5] [D loss: 0.3396] [G loss: 7.4132]\n",
      "[Epoch 34/100] [Batch 2/5] [D loss: 0.3458] [G loss: 5.4149]\n",
      "[Epoch 34/100] [Batch 3/5] [D loss: 0.3322] [G loss: 6.5967]\n",
      "[Epoch 34/100] [Batch 4/5] [D loss: 0.3258] [G loss: 6.7328]\n",
      "[Epoch 35/100] [Batch 0/5] [D loss: 0.3323] [G loss: 9.0976]\n",
      "[Epoch 35/100] [Batch 1/5] [D loss: 0.3566] [G loss: 5.5095]\n",
      "[Epoch 35/100] [Batch 2/5] [D loss: 0.3266] [G loss: 6.0835]\n",
      "[Epoch 35/100] [Batch 3/5] [D loss: 0.3821] [G loss: 5.0434]\n",
      "[Epoch 35/100] [Batch 4/5] [D loss: 0.3387] [G loss: 4.6261]\n",
      "[Epoch 36/100] [Batch 0/5] [D loss: 0.3421] [G loss: 4.9150]\n",
      "[Epoch 36/100] [Batch 1/5] [D loss: 0.3319] [G loss: 8.3654]\n",
      "[Epoch 36/100] [Batch 2/5] [D loss: 0.4446] [G loss: 6.9862]\n",
      "[Epoch 36/100] [Batch 3/5] [D loss: 0.3437] [G loss: 6.1704]\n",
      "[Epoch 36/100] [Batch 4/5] [D loss: 0.3836] [G loss: 4.8798]\n",
      "[Epoch 37/100] [Batch 0/5] [D loss: 0.3361] [G loss: 7.4865]\n",
      "[Epoch 37/100] [Batch 1/5] [D loss: 0.3322] [G loss: 5.4491]\n",
      "[Epoch 37/100] [Batch 2/5] [D loss: 0.5396] [G loss: 5.3939]\n",
      "[Epoch 37/100] [Batch 3/5] [D loss: 0.3527] [G loss: 5.5678]\n",
      "[Epoch 37/100] [Batch 4/5] [D loss: 0.3572] [G loss: 7.4819]\n",
      "[Epoch 38/100] [Batch 0/5] [D loss: 0.3254] [G loss: 7.8380]\n",
      "[Epoch 38/100] [Batch 1/5] [D loss: 0.3256] [G loss: 7.1529]\n",
      "[Epoch 38/100] [Batch 2/5] [D loss: 0.4586] [G loss: 5.8915]\n",
      "[Epoch 38/100] [Batch 3/5] [D loss: 0.3259] [G loss: 6.1625]\n",
      "[Epoch 38/100] [Batch 4/5] [D loss: 0.4005] [G loss: 10.2478]\n",
      "[Epoch 39/100] [Batch 0/5] [D loss: 0.3513] [G loss: 7.2392]\n",
      "[Epoch 39/100] [Batch 1/5] [D loss: 0.3253] [G loss: 8.8893]\n",
      "[Epoch 39/100] [Batch 2/5] [D loss: 0.4970] [G loss: 4.9432]\n",
      "[Epoch 39/100] [Batch 3/5] [D loss: 0.3692] [G loss: 4.8761]\n",
      "[Epoch 39/100] [Batch 4/5] [D loss: 0.3452] [G loss: 9.2437]\n",
      "[Epoch 40/100] [Batch 0/5] [D loss: 0.5535] [G loss: 3.6506]\n",
      "[Epoch 40/100] [Batch 1/5] [D loss: 0.4371] [G loss: 4.5851]\n",
      "[Epoch 40/100] [Batch 2/5] [D loss: 0.7230] [G loss: 9.0550]\n",
      "[Epoch 40/100] [Batch 3/5] [D loss: 0.5034] [G loss: 12.2899]\n",
      "[Epoch 40/100] [Batch 4/5] [D loss: 0.3575] [G loss: 7.8670]\n",
      "[Epoch 41/100] [Batch 0/5] [D loss: 0.3462] [G loss: 8.8535]\n",
      "[Epoch 41/100] [Batch 1/5] [D loss: 0.4031] [G loss: 8.9264]\n",
      "[Epoch 41/100] [Batch 2/5] [D loss: 0.4622] [G loss: 5.0951]\n",
      "[Epoch 41/100] [Batch 3/5] [D loss: 0.4820] [G loss: 6.7062]\n",
      "[Epoch 41/100] [Batch 4/5] [D loss: 0.7334] [G loss: 4.0222]\n",
      "[Epoch 42/100] [Batch 0/5] [D loss: 0.3973] [G loss: 4.0909]\n",
      "[Epoch 42/100] [Batch 1/5] [D loss: 0.4568] [G loss: 3.8018]\n",
      "[Epoch 42/100] [Batch 2/5] [D loss: 1.0330] [G loss: 5.1883]\n",
      "[Epoch 42/100] [Batch 3/5] [D loss: 0.4521] [G loss: 5.3627]\n",
      "[Epoch 42/100] [Batch 4/5] [D loss: 0.8129] [G loss: 2.0564]\n",
      "[Epoch 43/100] [Batch 0/5] [D loss: 0.6663] [G loss: 8.0244]\n",
      "[Epoch 43/100] [Batch 1/5] [D loss: 2.9777] [G loss: 10.2423]\n",
      "[Epoch 43/100] [Batch 2/5] [D loss: 0.3653] [G loss: 10.7524]\n",
      "[Epoch 43/100] [Batch 3/5] [D loss: 3.2268] [G loss: 9.1047]\n",
      "[Epoch 43/100] [Batch 4/5] [D loss: 2.8541] [G loss: 6.2871]\n",
      "[Epoch 44/100] [Batch 0/5] [D loss: 0.6253] [G loss: 3.8222]\n",
      "[Epoch 44/100] [Batch 1/5] [D loss: 1.5272] [G loss: 3.5242]\n",
      "[Epoch 44/100] [Batch 2/5] [D loss: 0.4796] [G loss: 3.7232]\n",
      "[Epoch 44/100] [Batch 3/5] [D loss: 0.3896] [G loss: 4.3708]\n",
      "[Epoch 44/100] [Batch 4/5] [D loss: 1.4971] [G loss: 1.5559]\n",
      "[Epoch 45/100] [Batch 0/5] [D loss: 4.3570] [G loss: 8.3761]\n",
      "[Epoch 45/100] [Batch 1/5] [D loss: 0.5673] [G loss: 6.9264]\n",
      "[Epoch 45/100] [Batch 2/5] [D loss: 0.7781] [G loss: 7.2371]\n",
      "[Epoch 45/100] [Batch 3/5] [D loss: 0.3509] [G loss: 5.4645]\n",
      "[Epoch 45/100] [Batch 4/5] [D loss: 0.3297] [G loss: 8.2624]\n",
      "[Epoch 46/100] [Batch 0/5] [D loss: 0.3278] [G loss: 8.1981]\n",
      "[Epoch 46/100] [Batch 1/5] [D loss: 0.4246] [G loss: 4.5964]\n",
      "[Epoch 46/100] [Batch 2/5] [D loss: 0.6260] [G loss: 5.0804]\n",
      "[Epoch 46/100] [Batch 3/5] [D loss: 0.7123] [G loss: 3.9454]\n",
      "[Epoch 46/100] [Batch 4/5] [D loss: 0.4096] [G loss: 3.7918]\n",
      "[Epoch 47/100] [Batch 0/5] [D loss: 0.4206] [G loss: 5.0761]\n",
      "[Epoch 47/100] [Batch 1/5] [D loss: 0.4725] [G loss: 3.9232]\n",
      "[Epoch 47/100] [Batch 2/5] [D loss: 0.4657] [G loss: 5.6087]\n",
      "[Epoch 47/100] [Batch 3/5] [D loss: 0.4897] [G loss: 2.5140]\n",
      "[Epoch 47/100] [Batch 4/5] [D loss: 0.4511] [G loss: 4.5087]\n",
      "[Epoch 48/100] [Batch 0/5] [D loss: 0.5468] [G loss: 4.5248]\n",
      "[Epoch 48/100] [Batch 1/5] [D loss: 0.5876] [G loss: 5.0404]\n",
      "[Epoch 48/100] [Batch 2/5] [D loss: 0.3603] [G loss: 5.1016]\n",
      "[Epoch 48/100] [Batch 3/5] [D loss: 1.5229] [G loss: 3.4659]\n",
      "[Epoch 48/100] [Batch 4/5] [D loss: 0.4634] [G loss: 4.5751]\n",
      "[Epoch 49/100] [Batch 0/5] [D loss: 0.9709] [G loss: 7.2456]\n",
      "[Epoch 49/100] [Batch 1/5] [D loss: 0.3572] [G loss: 9.1236]\n",
      "[Epoch 49/100] [Batch 2/5] [D loss: 0.3328] [G loss: 6.6415]\n",
      "[Epoch 49/100] [Batch 3/5] [D loss: 0.5382] [G loss: 4.2410]\n",
      "[Epoch 49/100] [Batch 4/5] [D loss: 0.5257] [G loss: 7.1641]\n",
      "[Epoch 50/100] [Batch 0/5] [D loss: 0.3304] [G loss: 5.8917]\n",
      "[Epoch 50/100] [Batch 1/5] [D loss: 0.9117] [G loss: 4.7717]\n",
      "[Epoch 50/100] [Batch 2/5] [D loss: 1.5059] [G loss: 8.2964]\n",
      "[Epoch 50/100] [Batch 3/5] [D loss: 0.4765] [G loss: 5.6438]\n",
      "[Epoch 50/100] [Batch 4/5] [D loss: 0.3789] [G loss: 9.7138]\n",
      "[Epoch 51/100] [Batch 0/5] [D loss: 0.3362] [G loss: 11.9884]\n",
      "[Epoch 51/100] [Batch 1/5] [D loss: 0.6770] [G loss: 8.5569]\n",
      "[Epoch 51/100] [Batch 2/5] [D loss: 1.8930] [G loss: 4.6733]\n",
      "[Epoch 51/100] [Batch 3/5] [D loss: 0.9805] [G loss: 5.9724]\n",
      "[Epoch 51/100] [Batch 4/5] [D loss: 1.0500] [G loss: 6.2865]\n",
      "[Epoch 52/100] [Batch 0/5] [D loss: 0.5576] [G loss: 7.9367]\n",
      "[Epoch 52/100] [Batch 1/5] [D loss: 0.7760] [G loss: 3.5769]\n",
      "[Epoch 52/100] [Batch 2/5] [D loss: 0.4890] [G loss: 4.6125]\n",
      "[Epoch 52/100] [Batch 3/5] [D loss: 0.3959] [G loss: 3.8975]\n",
      "[Epoch 52/100] [Batch 4/5] [D loss: 1.0890] [G loss: 3.7567]\n",
      "[Epoch 53/100] [Batch 0/5] [D loss: 0.3442] [G loss: 4.6672]\n",
      "[Epoch 53/100] [Batch 1/5] [D loss: 0.3798] [G loss: 4.2548]\n",
      "[Epoch 53/100] [Batch 2/5] [D loss: 0.8731] [G loss: 4.5635]\n",
      "[Epoch 53/100] [Batch 3/5] [D loss: 0.5302] [G loss: 2.3152]\n",
      "[Epoch 53/100] [Batch 4/5] [D loss: 0.4378] [G loss: 4.3139]\n",
      "[Epoch 54/100] [Batch 0/5] [D loss: 0.7562] [G loss: 6.4392]\n",
      "[Epoch 54/100] [Batch 1/5] [D loss: 0.6919] [G loss: 4.4053]\n",
      "[Epoch 54/100] [Batch 2/5] [D loss: 0.5168] [G loss: 4.7221]\n",
      "[Epoch 54/100] [Batch 3/5] [D loss: 0.4413] [G loss: 4.4406]\n",
      "[Epoch 54/100] [Batch 4/5] [D loss: 0.4022] [G loss: 3.3449]\n",
      "[Epoch 55/100] [Batch 0/5] [D loss: 0.3357] [G loss: 4.5697]\n",
      "[Epoch 55/100] [Batch 1/5] [D loss: 0.6111] [G loss: 4.3343]\n",
      "[Epoch 55/100] [Batch 2/5] [D loss: 0.4187] [G loss: 6.9275]\n",
      "[Epoch 55/100] [Batch 3/5] [D loss: 0.5230] [G loss: 6.7651]\n",
      "[Epoch 55/100] [Batch 4/5] [D loss: 0.3822] [G loss: 3.8817]\n",
      "[Epoch 56/100] [Batch 0/5] [D loss: 0.3631] [G loss: 4.4756]\n",
      "[Epoch 56/100] [Batch 1/5] [D loss: 0.3712] [G loss: 3.5009]\n",
      "[Epoch 56/100] [Batch 2/5] [D loss: 0.3314] [G loss: 6.7597]\n",
      "[Epoch 56/100] [Batch 3/5] [D loss: 0.3306] [G loss: 4.8944]\n",
      "[Epoch 56/100] [Batch 4/5] [D loss: 0.3956] [G loss: 3.4570]\n",
      "[Epoch 57/100] [Batch 0/5] [D loss: 0.4154] [G loss: 3.5402]\n",
      "[Epoch 57/100] [Batch 1/5] [D loss: 0.3356] [G loss: 9.5378]\n",
      "[Epoch 57/100] [Batch 2/5] [D loss: 0.3454] [G loss: 7.9896]\n",
      "[Epoch 57/100] [Batch 3/5] [D loss: 0.3420] [G loss: 4.7901]\n",
      "[Epoch 57/100] [Batch 4/5] [D loss: 0.3529] [G loss: 3.6879]\n",
      "[Epoch 58/100] [Batch 0/5] [D loss: 0.3384] [G loss: 4.9744]\n",
      "[Epoch 58/100] [Batch 1/5] [D loss: 0.4522] [G loss: 4.3384]\n",
      "[Epoch 58/100] [Batch 2/5] [D loss: 0.6489] [G loss: 7.5775]\n",
      "[Epoch 58/100] [Batch 3/5] [D loss: 0.4630] [G loss: 6.5652]\n",
      "[Epoch 58/100] [Batch 4/5] [D loss: 0.3581] [G loss: 6.6926]\n",
      "[Epoch 59/100] [Batch 0/5] [D loss: 0.4930] [G loss: 7.4353]\n",
      "[Epoch 59/100] [Batch 1/5] [D loss: 0.4744] [G loss: 8.4840]\n",
      "[Epoch 59/100] [Batch 2/5] [D loss: 0.3978] [G loss: 5.3036]\n",
      "[Epoch 59/100] [Batch 3/5] [D loss: 0.4870] [G loss: 4.2853]\n",
      "[Epoch 59/100] [Batch 4/5] [D loss: 0.4494] [G loss: 4.2470]\n",
      "[Epoch 60/100] [Batch 0/5] [D loss: 0.3296] [G loss: 6.5100]\n",
      "[Epoch 60/100] [Batch 1/5] [D loss: 0.3766] [G loss: 4.5733]\n",
      "[Epoch 60/100] [Batch 2/5] [D loss: 0.4307] [G loss: 5.0528]\n",
      "[Epoch 60/100] [Batch 3/5] [D loss: 0.3761] [G loss: 3.6268]\n",
      "[Epoch 60/100] [Batch 4/5] [D loss: 0.4270] [G loss: 5.7516]\n",
      "[Epoch 61/100] [Batch 0/5] [D loss: 0.4503] [G loss: 4.4011]\n",
      "[Epoch 61/100] [Batch 1/5] [D loss: 0.3694] [G loss: 4.6099]\n",
      "[Epoch 61/100] [Batch 2/5] [D loss: 0.4109] [G loss: 5.1888]\n",
      "[Epoch 61/100] [Batch 3/5] [D loss: 0.5125] [G loss: 4.2322]\n",
      "[Epoch 61/100] [Batch 4/5] [D loss: 0.3453] [G loss: 8.8570]\n",
      "[Epoch 62/100] [Batch 0/5] [D loss: 0.5672] [G loss: 5.0687]\n",
      "[Epoch 62/100] [Batch 1/5] [D loss: 0.3389] [G loss: 6.7226]\n",
      "[Epoch 62/100] [Batch 2/5] [D loss: 1.2112] [G loss: 11.3014]\n",
      "[Epoch 62/100] [Batch 3/5] [D loss: 0.6322] [G loss: 11.3303]\n",
      "[Epoch 62/100] [Batch 4/5] [D loss: 0.3275] [G loss: 8.0114]\n",
      "[Epoch 63/100] [Batch 0/5] [D loss: 0.3272] [G loss: 9.6660]\n",
      "[Epoch 63/100] [Batch 1/5] [D loss: 0.4256] [G loss: 6.7843]\n",
      "[Epoch 63/100] [Batch 2/5] [D loss: 0.4874] [G loss: 7.9624]\n",
      "[Epoch 63/100] [Batch 3/5] [D loss: 0.5082] [G loss: 6.6377]\n",
      "[Epoch 63/100] [Batch 4/5] [D loss: 0.3810] [G loss: 8.1186]\n",
      "[Epoch 64/100] [Batch 0/5] [D loss: 0.4563] [G loss: 3.5707]\n",
      "[Epoch 64/100] [Batch 1/5] [D loss: 0.4038] [G loss: 7.7827]\n",
      "[Epoch 64/100] [Batch 2/5] [D loss: 0.4795] [G loss: 5.3204]\n",
      "[Epoch 64/100] [Batch 3/5] [D loss: 0.3452] [G loss: 6.8377]\n",
      "[Epoch 64/100] [Batch 4/5] [D loss: 0.4698] [G loss: 3.4884]\n",
      "[Epoch 65/100] [Batch 0/5] [D loss: 0.3279] [G loss: 6.6171]\n",
      "[Epoch 65/100] [Batch 1/5] [D loss: 0.3684] [G loss: 3.7332]\n",
      "[Epoch 65/100] [Batch 2/5] [D loss: 0.3373] [G loss: 4.7539]\n",
      "[Epoch 65/100] [Batch 3/5] [D loss: 0.4333] [G loss: 4.9255]\n",
      "[Epoch 65/100] [Batch 4/5] [D loss: 0.3297] [G loss: 7.4712]\n",
      "[Epoch 66/100] [Batch 0/5] [D loss: 0.4352] [G loss: 3.5541]\n",
      "[Epoch 66/100] [Batch 1/5] [D loss: 0.3479] [G loss: 4.3807]\n",
      "[Epoch 66/100] [Batch 2/5] [D loss: 0.3251] [G loss: 9.8947]\n",
      "[Epoch 66/100] [Batch 3/5] [D loss: 0.3272] [G loss: 5.9816]\n",
      "[Epoch 66/100] [Batch 4/5] [D loss: 0.4642] [G loss: 4.4133]\n",
      "[Epoch 67/100] [Batch 0/5] [D loss: 0.3345] [G loss: 6.2682]\n",
      "[Epoch 67/100] [Batch 1/5] [D loss: 0.3673] [G loss: 6.0860]\n",
      "[Epoch 67/100] [Batch 2/5] [D loss: 0.3395] [G loss: 4.6165]\n",
      "[Epoch 67/100] [Batch 3/5] [D loss: 0.4796] [G loss: 2.9704]\n",
      "[Epoch 67/100] [Batch 4/5] [D loss: 0.3254] [G loss: 7.6386]\n",
      "[Epoch 68/100] [Batch 0/5] [D loss: 0.3679] [G loss: 4.3250]\n",
      "[Epoch 68/100] [Batch 1/5] [D loss: 0.7878] [G loss: 6.9726]\n",
      "[Epoch 68/100] [Batch 2/5] [D loss: 0.3262] [G loss: 10.5069]\n",
      "[Epoch 68/100] [Batch 3/5] [D loss: 0.3323] [G loss: 7.5715]\n",
      "[Epoch 68/100] [Batch 4/5] [D loss: 0.9713] [G loss: 8.8612]\n",
      "[Epoch 69/100] [Batch 0/5] [D loss: 0.4072] [G loss: 6.2554]\n",
      "[Epoch 69/100] [Batch 1/5] [D loss: 0.3312] [G loss: 9.6700]\n",
      "[Epoch 69/100] [Batch 2/5] [D loss: 0.3398] [G loss: 4.7816]\n",
      "[Epoch 69/100] [Batch 3/5] [D loss: 0.6165] [G loss: 6.3450]\n",
      "[Epoch 69/100] [Batch 4/5] [D loss: 0.8401] [G loss: 4.3938]\n",
      "[Epoch 70/100] [Batch 0/5] [D loss: 0.5610] [G loss: 7.5164]\n",
      "[Epoch 70/100] [Batch 1/5] [D loss: 0.3657] [G loss: 3.7497]\n",
      "[Epoch 70/100] [Batch 2/5] [D loss: 2.2229] [G loss: 9.3643]\n",
      "[Epoch 70/100] [Batch 3/5] [D loss: 0.6799] [G loss: 7.2828]\n",
      "[Epoch 70/100] [Batch 4/5] [D loss: 0.3275] [G loss: 5.6566]\n",
      "[Epoch 71/100] [Batch 0/5] [D loss: 0.4366] [G loss: 7.9878]\n",
      "[Epoch 71/100] [Batch 1/5] [D loss: 0.3557] [G loss: 8.0856]\n",
      "[Epoch 71/100] [Batch 2/5] [D loss: 0.5866] [G loss: 8.6639]\n",
      "[Epoch 71/100] [Batch 3/5] [D loss: 1.7562] [G loss: 2.8262]\n",
      "[Epoch 71/100] [Batch 4/5] [D loss: 0.3436] [G loss: 3.7220]\n",
      "[Epoch 72/100] [Batch 0/5] [D loss: 0.5155] [G loss: 4.0122]\n",
      "[Epoch 72/100] [Batch 1/5] [D loss: 0.3890] [G loss: 3.8959]\n",
      "[Epoch 72/100] [Batch 2/5] [D loss: 0.9943] [G loss: 4.2074]\n",
      "[Epoch 72/100] [Batch 3/5] [D loss: 0.5143] [G loss: 4.2245]\n",
      "[Epoch 72/100] [Batch 4/5] [D loss: 1.4705] [G loss: 7.6934]\n",
      "[Epoch 73/100] [Batch 0/5] [D loss: 1.0078] [G loss: 5.1330]\n",
      "[Epoch 73/100] [Batch 1/5] [D loss: 1.5720] [G loss: 6.7867]\n",
      "[Epoch 73/100] [Batch 2/5] [D loss: 0.3952] [G loss: 8.0989]\n",
      "[Epoch 73/100] [Batch 3/5] [D loss: 0.4361] [G loss: 7.5410]\n",
      "[Epoch 73/100] [Batch 4/5] [D loss: 3.6160] [G loss: 7.6590]\n",
      "[Epoch 74/100] [Batch 0/5] [D loss: 0.3560] [G loss: 3.8915]\n",
      "[Epoch 74/100] [Batch 1/5] [D loss: 0.3567] [G loss: 4.1451]\n",
      "[Epoch 74/100] [Batch 2/5] [D loss: 0.7763] [G loss: 5.4209]\n",
      "[Epoch 74/100] [Batch 3/5] [D loss: 0.5617] [G loss: 3.3540]\n",
      "[Epoch 74/100] [Batch 4/5] [D loss: 0.3873] [G loss: 4.5783]\n",
      "[Epoch 75/100] [Batch 0/5] [D loss: 2.6428] [G loss: 2.9944]\n",
      "[Epoch 75/100] [Batch 1/5] [D loss: 0.3455] [G loss: 5.1758]\n",
      "[Epoch 75/100] [Batch 2/5] [D loss: 0.7376] [G loss: 4.8107]\n",
      "[Epoch 75/100] [Batch 3/5] [D loss: 0.3873] [G loss: 3.5849]\n",
      "[Epoch 75/100] [Batch 4/5] [D loss: 0.3362] [G loss: 4.2739]\n",
      "[Epoch 76/100] [Batch 0/5] [D loss: 0.3582] [G loss: 3.3131]\n",
      "[Epoch 76/100] [Batch 1/5] [D loss: 0.3932] [G loss: 3.5237]\n",
      "[Epoch 76/100] [Batch 2/5] [D loss: 0.3655] [G loss: 5.5403]\n",
      "[Epoch 76/100] [Batch 3/5] [D loss: 0.3665] [G loss: 3.7347]\n",
      "[Epoch 76/100] [Batch 4/5] [D loss: 0.8278] [G loss: 2.4718]\n",
      "[Epoch 77/100] [Batch 0/5] [D loss: 0.4193] [G loss: 2.9910]\n",
      "[Epoch 77/100] [Batch 1/5] [D loss: 0.7494] [G loss: 3.1965]\n",
      "[Epoch 77/100] [Batch 2/5] [D loss: 0.4563] [G loss: 4.3893]\n",
      "[Epoch 77/100] [Batch 3/5] [D loss: 0.3355] [G loss: 6.9567]\n",
      "[Epoch 77/100] [Batch 4/5] [D loss: 0.3757] [G loss: 3.8666]\n",
      "[Epoch 78/100] [Batch 0/5] [D loss: 1.3795] [G loss: 4.2313]\n",
      "[Epoch 78/100] [Batch 1/5] [D loss: 0.3592] [G loss: 3.1923]\n",
      "[Epoch 78/100] [Batch 2/5] [D loss: 0.3759] [G loss: 4.5854]\n",
      "[Epoch 78/100] [Batch 3/5] [D loss: 0.4536] [G loss: 4.0375]\n",
      "[Epoch 78/100] [Batch 4/5] [D loss: 0.6636] [G loss: 5.1286]\n",
      "[Epoch 79/100] [Batch 0/5] [D loss: 0.3382] [G loss: 8.8043]\n",
      "[Epoch 79/100] [Batch 1/5] [D loss: 0.4418] [G loss: 6.9863]\n",
      "[Epoch 79/100] [Batch 2/5] [D loss: 0.4312] [G loss: 4.3557]\n",
      "[Epoch 79/100] [Batch 3/5] [D loss: 0.3733] [G loss: 3.2380]\n",
      "[Epoch 79/100] [Batch 4/5] [D loss: 0.4314] [G loss: 3.3283]\n",
      "[Epoch 80/100] [Batch 0/5] [D loss: 0.3257] [G loss: 7.6624]\n",
      "[Epoch 80/100] [Batch 1/5] [D loss: 0.4321] [G loss: 9.4407]\n",
      "[Epoch 80/100] [Batch 2/5] [D loss: 0.3835] [G loss: 3.8338]\n",
      "[Epoch 80/100] [Batch 3/5] [D loss: 0.7394] [G loss: 3.9796]\n",
      "[Epoch 80/100] [Batch 4/5] [D loss: 0.5471] [G loss: 3.7897]\n",
      "[Epoch 81/100] [Batch 0/5] [D loss: 0.3359] [G loss: 8.9526]\n",
      "[Epoch 81/100] [Batch 1/5] [D loss: 0.3839] [G loss: 3.8922]\n",
      "[Epoch 81/100] [Batch 2/5] [D loss: 0.3309] [G loss: 6.2731]\n",
      "[Epoch 81/100] [Batch 3/5] [D loss: 0.4004] [G loss: 3.3118]\n",
      "[Epoch 81/100] [Batch 4/5] [D loss: 0.3447] [G loss: 4.1836]\n",
      "[Epoch 82/100] [Batch 0/5] [D loss: 0.3387] [G loss: 4.4470]\n",
      "[Epoch 82/100] [Batch 1/5] [D loss: 0.3492] [G loss: 3.9992]\n",
      "[Epoch 82/100] [Batch 2/5] [D loss: 0.6410] [G loss: 4.4847]\n",
      "[Epoch 82/100] [Batch 3/5] [D loss: 0.3947] [G loss: 7.4897]\n",
      "[Epoch 82/100] [Batch 4/5] [D loss: 0.3742] [G loss: 4.0571]\n",
      "[Epoch 83/100] [Batch 0/5] [D loss: 0.3467] [G loss: 4.2889]\n",
      "[Epoch 83/100] [Batch 1/5] [D loss: 0.3737] [G loss: 3.6866]\n",
      "[Epoch 83/100] [Batch 2/5] [D loss: 0.6119] [G loss: 7.3699]\n",
      "[Epoch 83/100] [Batch 3/5] [D loss: 0.3326] [G loss: 4.0923]\n",
      "[Epoch 83/100] [Batch 4/5] [D loss: 0.3321] [G loss: 5.0625]\n",
      "[Epoch 84/100] [Batch 0/5] [D loss: 3.0418] [G loss: 9.3659]\n",
      "[Epoch 84/100] [Batch 1/5] [D loss: 0.3351] [G loss: 8.3318]\n",
      "[Epoch 84/100] [Batch 2/5] [D loss: 0.8421] [G loss: 6.9151]\n",
      "[Epoch 84/100] [Batch 3/5] [D loss: 0.6154] [G loss: 12.1896]\n",
      "[Epoch 84/100] [Batch 4/5] [D loss: 0.6707] [G loss: 4.0183]\n",
      "[Epoch 85/100] [Batch 0/5] [D loss: 0.8827] [G loss: 2.8294]\n",
      "[Epoch 85/100] [Batch 1/5] [D loss: 0.3622] [G loss: 4.8404]\n",
      "[Epoch 85/100] [Batch 2/5] [D loss: 0.4455] [G loss: 5.5014]\n",
      "[Epoch 85/100] [Batch 3/5] [D loss: 0.4882] [G loss: 9.8801]\n",
      "[Epoch 85/100] [Batch 4/5] [D loss: 1.3790] [G loss: 6.9297]\n",
      "[Epoch 86/100] [Batch 0/5] [D loss: 0.3778] [G loss: 6.2445]\n",
      "[Epoch 86/100] [Batch 1/5] [D loss: 0.6140] [G loss: 3.5449]\n",
      "[Epoch 86/100] [Batch 2/5] [D loss: 0.8966] [G loss: 4.2327]\n",
      "[Epoch 86/100] [Batch 3/5] [D loss: 0.3848] [G loss: 3.6007]\n",
      "[Epoch 86/100] [Batch 4/5] [D loss: 0.6911] [G loss: 2.1297]\n",
      "[Epoch 87/100] [Batch 0/5] [D loss: 0.3635] [G loss: 3.7587]\n",
      "[Epoch 87/100] [Batch 1/5] [D loss: 0.6866] [G loss: 3.9613]\n",
      "[Epoch 87/100] [Batch 2/5] [D loss: 0.8418] [G loss: 4.6754]\n",
      "[Epoch 87/100] [Batch 3/5] [D loss: 0.3544] [G loss: 4.1665]\n",
      "[Epoch 87/100] [Batch 4/5] [D loss: 1.5896] [G loss: 1.8827]\n",
      "[Epoch 88/100] [Batch 0/5] [D loss: 1.3285] [G loss: 5.3408]\n",
      "[Epoch 88/100] [Batch 1/5] [D loss: 0.3309] [G loss: 5.7636]\n",
      "[Epoch 88/100] [Batch 2/5] [D loss: 0.6655] [G loss: 4.9670]\n",
      "[Epoch 88/100] [Batch 3/5] [D loss: 0.3641] [G loss: 5.9980]\n",
      "[Epoch 88/100] [Batch 4/5] [D loss: 0.6262] [G loss: 6.6574]\n",
      "[Epoch 89/100] [Batch 0/5] [D loss: 0.3410] [G loss: 6.6941]\n",
      "[Epoch 89/100] [Batch 1/5] [D loss: 0.5474] [G loss: 3.5272]\n",
      "[Epoch 89/100] [Batch 2/5] [D loss: 0.4924] [G loss: 4.4629]\n",
      "[Epoch 89/100] [Batch 3/5] [D loss: 0.3306] [G loss: 5.2631]\n",
      "[Epoch 89/100] [Batch 4/5] [D loss: 0.3775] [G loss: 4.8428]\n",
      "[Epoch 90/100] [Batch 0/5] [D loss: 0.4169] [G loss: 2.7872]\n",
      "[Epoch 90/100] [Batch 1/5] [D loss: 0.4911] [G loss: 3.2217]\n",
      "[Epoch 90/100] [Batch 2/5] [D loss: 0.3645] [G loss: 7.1506]\n",
      "[Epoch 90/100] [Batch 3/5] [D loss: 0.3455] [G loss: 5.1518]\n",
      "[Epoch 90/100] [Batch 4/5] [D loss: 0.3868] [G loss: 3.4646]\n",
      "[Epoch 91/100] [Batch 0/5] [D loss: 0.4611] [G loss: 8.3594]\n",
      "[Epoch 91/100] [Batch 1/5] [D loss: 0.3653] [G loss: 5.2916]\n",
      "[Epoch 91/100] [Batch 2/5] [D loss: 0.3288] [G loss: 5.6223]\n",
      "[Epoch 91/100] [Batch 3/5] [D loss: 0.4090] [G loss: 3.4794]\n",
      "[Epoch 91/100] [Batch 4/5] [D loss: 0.3372] [G loss: 4.4878]\n",
      "[Epoch 92/100] [Batch 0/5] [D loss: 0.4708] [G loss: 5.5349]\n",
      "[Epoch 92/100] [Batch 1/5] [D loss: 0.3533] [G loss: 4.1009]\n",
      "[Epoch 92/100] [Batch 2/5] [D loss: 0.3408] [G loss: 4.0034]\n",
      "[Epoch 92/100] [Batch 3/5] [D loss: 0.3633] [G loss: 3.7895]\n",
      "[Epoch 92/100] [Batch 4/5] [D loss: 0.3325] [G loss: 4.7792]\n",
      "[Epoch 93/100] [Batch 0/5] [D loss: 0.4337] [G loss: 3.3297]\n",
      "[Epoch 93/100] [Batch 1/5] [D loss: 0.3495] [G loss: 4.2546]\n",
      "[Epoch 93/100] [Batch 2/5] [D loss: 0.3783] [G loss: 3.8290]\n",
      "[Epoch 93/100] [Batch 3/5] [D loss: 0.4218] [G loss: 3.4058]\n",
      "[Epoch 93/100] [Batch 4/5] [D loss: 0.3618] [G loss: 3.6272]\n",
      "[Epoch 94/100] [Batch 0/5] [D loss: 0.3384] [G loss: 4.6211]\n",
      "[Epoch 94/100] [Batch 1/5] [D loss: 0.3282] [G loss: 5.4289]\n",
      "[Epoch 94/100] [Batch 2/5] [D loss: 0.3699] [G loss: 3.9579]\n",
      "[Epoch 94/100] [Batch 3/5] [D loss: 0.4069] [G loss: 3.5106]\n",
      "[Epoch 94/100] [Batch 4/5] [D loss: 0.3504] [G loss: 4.0398]\n",
      "[Epoch 95/100] [Batch 0/5] [D loss: 0.3515] [G loss: 5.3453]\n",
      "[Epoch 95/100] [Batch 1/5] [D loss: 0.3660] [G loss: 3.4399]\n",
      "[Epoch 95/100] [Batch 2/5] [D loss: 0.4526] [G loss: 3.5076]\n",
      "[Epoch 95/100] [Batch 3/5] [D loss: 0.3874] [G loss: 3.9280]\n",
      "[Epoch 95/100] [Batch 4/5] [D loss: 0.5302] [G loss: 3.0092]\n",
      "[Epoch 96/100] [Batch 0/5] [D loss: 0.3298] [G loss: 5.0441]\n",
      "[Epoch 96/100] [Batch 1/5] [D loss: 0.3371] [G loss: 5.6149]\n",
      "[Epoch 96/100] [Batch 2/5] [D loss: 0.4404] [G loss: 3.9877]\n",
      "[Epoch 96/100] [Batch 3/5] [D loss: 0.4644] [G loss: 4.3258]\n",
      "[Epoch 96/100] [Batch 4/5] [D loss: 0.3268] [G loss: 7.2237]\n",
      "[Epoch 97/100] [Batch 0/5] [D loss: 0.3280] [G loss: 6.3697]\n",
      "[Epoch 97/100] [Batch 1/5] [D loss: 0.4212] [G loss: 3.5687]\n",
      "[Epoch 97/100] [Batch 2/5] [D loss: 0.3264] [G loss: 6.7049]\n",
      "[Epoch 97/100] [Batch 3/5] [D loss: 0.3624] [G loss: 5.4828]\n",
      "[Epoch 97/100] [Batch 4/5] [D loss: 0.3398] [G loss: 4.1369]\n",
      "[Epoch 98/100] [Batch 0/5] [D loss: 0.4361] [G loss: 3.8273]\n",
      "[Epoch 98/100] [Batch 1/5] [D loss: 0.3402] [G loss: 4.5071]\n",
      "[Epoch 98/100] [Batch 2/5] [D loss: 0.4023] [G loss: 4.5508]\n",
      "[Epoch 98/100] [Batch 3/5] [D loss: 0.3523] [G loss: 5.8528]\n",
      "[Epoch 98/100] [Batch 4/5] [D loss: 0.3581] [G loss: 4.7098]\n",
      "[Epoch 99/100] [Batch 0/5] [D loss: 0.3702] [G loss: 4.2580]\n",
      "[Epoch 99/100] [Batch 1/5] [D loss: 0.3372] [G loss: 6.3267]\n",
      "[Epoch 99/100] [Batch 2/5] [D loss: 0.3434] [G loss: 3.9904]\n",
      "[Epoch 99/100] [Batch 3/5] [D loss: 0.3335] [G loss: 5.8171]\n",
      "[Epoch 99/100] [Batch 4/5] [D loss: 0.3904] [G loss: 3.6614]\n",
      "[Epoch 100/100] [Batch 0/5] [D loss: 0.3759] [G loss: 3.7225]\n",
      "[Epoch 100/100] [Batch 1/5] [D loss: 0.3421] [G loss: 4.4576]\n",
      "[Epoch 100/100] [Batch 2/5] [D loss: 0.4800] [G loss: 4.6520]\n",
      "[Epoch 100/100] [Batch 3/5] [D loss: 0.3931] [G loss: 4.3817]\n",
      "[Epoch 100/100] [Batch 4/5] [D loss: 0.3366] [G loss: 7.0295]\n"
     ]
    }
   ],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.ConvTranspose2d(latent_dim, 512, 4, 1, 0),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(128, img_channels, 4, 2, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "# ========== Discriminator ==========\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(img_channels, 128, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(128, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 512, 4, 2, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(512, 1, 4, 1, 0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        out = self.model(img)\n",
    "        return out.view(-1, 1)\n",
    "\n",
    "# ========== Model Initialization ==========\n",
    "model_G = Generator().to(device)\n",
    "model_D = Discriminator().to(device)\n",
    "\n",
    "# ========== Loss & Optimizers ==========\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer_G = torch.optim.Adam(model_G.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizer_D = torch.optim.Adam(model_D.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "# ========== Fixed Noise ==========\n",
    "fixed_noise = torch.randn(64, latent_dim, 1, 1, device=device)\n",
    "\n",
    "# ========== Training Loop ==========\n",
    "k = 0\n",
    "for epoch in range(epochs):\n",
    "    for i, (real_imgs, _) in enumerate(train_loader):\n",
    "        current_batch = real_imgs.size(0)\n",
    "        real_imgs = real_imgs.to(device)\n",
    "\n",
    "        # Real and fake labels\n",
    "        real_label = torch.full((current_batch, 1), 0.9, device=device)\n",
    "        fake_label = torch.zeros(current_batch, 1, device=device)\n",
    "\n",
    "        # ======== Train Discriminator ========\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        real_out = model_D(real_imgs)\n",
    "        real_loss = loss_fn(real_out, real_label)\n",
    "\n",
    "        noise = torch.randn(current_batch, latent_dim, 1, 1, device=device)\n",
    "        fake_imgs = model_G(noise)\n",
    "        fake_out = model_D(fake_imgs.detach())\n",
    "        fake_loss = loss_fn(fake_out, fake_label)\n",
    "\n",
    "        d_loss = real_loss + fake_loss\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # ======== Train Generator ========\n",
    "        optimizer_G.zero_grad()\n",
    "        output = model_D(fake_imgs)\n",
    "        g_loss = loss_fn(output, real_label)\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ======== Logging (optional but helpful) ========\n",
    "        if i % 1 == 0:\n",
    "            print(f\"[Epoch {epoch+1}/{epochs}] [Batch {i}/{len(train_loader)}] \"\n",
    "                  f\"[D loss: {d_loss.item():.4f}] [G loss: {g_loss.item():.4f}]\")\n",
    "\n",
    "        # ======== Save Sample Images ========\n",
    "        if i % 1 == 0:\n",
    "            with torch.no_grad():\n",
    "                fake_sample = model_G(fixed_noise).detach().cpu()\n",
    "                save_image(fake_sample, f\"dcgan_outputs/fake_{epoch:03d}_{k:04d}.png\", normalize=True)\n",
    "            k += 1\n",
    "\n",
    "# ========== Save Final Model ==========\n",
    "torch.save(model_G.state_dict(), \"dcgan_generator.pth\")\n",
    "torch.save(model_D.state_dict(), \"dcgan_discriminator.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a78a358",
   "metadata": {
    "papermill": {
     "duration": 0.016945,
     "end_time": "2025-08-08T15:58:55.248865",
     "exception": false,
     "start_time": "2025-08-08T15:58:55.231920",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8033906,
     "sourceId": 12711278,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 125.256989,
   "end_time": "2025-08-08T15:58:57.546311",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-08T15:56:52.289322",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
